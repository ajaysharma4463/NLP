{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important one machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create a machine learniig model by scikit learn\n",
    "# so we have two variables    --lenght and puntuation\n",
    "# it look like punctuation and length of spam is high if we see in matplotlib \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1118e817dcf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# before executing we need to specify x and y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "#step 2\n",
    "train_test_split(x,y)  # before executing we need to specify x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5af330fe25e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# x is our feature data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"length\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"punct\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# y is our label data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#step 3\n",
    "# x is our feature data\n",
    "x=df[[\"length\",\"punct\"]]\n",
    "# y is our label data\n",
    "y=df[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4\n",
    "train_test_split(x,y)   #shift +tab for copy\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # randow will pic  \n",
    "#data becasue  if our data is sorted then it will\n",
    "#pic first  10 row out of 30 from top which will be not good apprach to do machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5\n",
    "x_train.shape()  # output would be (3900,2)   ,3900 row and 2 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape()  # it has (1672,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we have the data , so we need to use machine learnign algorithm\n",
    "# lets use LogisticRegression model\n",
    "# we can apply any method but we will find which best of suitable for the best\n",
    " from sklearn.linear_model import LogisticRegression\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model=LogisticRegression(solver=\"lbfgs\")# lot of deault value in this algorthm\n",
    "# object creation is done \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now its time to fit the taining data\n",
    "lr_model.fit(x_train,y_train)  # we dont need to assign in variable i am just calling fir function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets test our data \n",
    "from sklearn import metrics\n",
    "# now model has data  so it is ready to predict\n",
    "# but we will put a new dat which we took from dataset\n",
    "# it is x_test data\n",
    "prediction=lr_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction  # output will be  array([\"ham\",....])\n",
    "# it has ham (legimate data,corrdct data) and spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use confusion matrics\n",
    "print(metrics.confusion_matrix(y_test,prediction))\n",
    "# output will be \n",
    "1404  44\n",
    "219    5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dataframe(c),index=['ham',\"spam\"],column=['ham',\"spam\"])\n",
    "             \n",
    "# output will be \n",
    "      ham spam        \n",
    "ham  1404  44\n",
    "spam 219    5\n",
    "             \n",
    "# it look like i only correctly find out 5 spam\n",
    "# i correcly classify ham as 1404\n",
    "#44 and 219 are incorrect\n",
    "# i think 4 and 219  should be closer to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i can use clasification \n",
    "print(metrics.classification_report(y_test,prediction))\n",
    "# outpu will be \n",
    "     precision   recall    Fi-score      support\n",
    "ham  0.87        0.97      0.91           1448\n",
    "spam 0.10         0.02      0.04            224\n",
    "avg/total\n",
    "     0.76        0.84      0.80            1672\n",
    "    \n",
    "    \n",
    "    # so its look like ur model is good to deteted ham\n",
    "    # but it is worse for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the accuracy of our model\n",
    "print(metrics.accuracy_score(y_test,predictions))\n",
    "# output will be \n",
    "0.8437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_model=MultinomialNB()\n",
    "nb_model.fit(x_train,Y_train)\n",
    "prediction=lr_model.predict(x_test)\n",
    "print(metrics.confusion_matrix(y_test,prediction))\n",
    "# output of last code is:\n",
    "\n",
    "1438  10\n",
    "224   0\n",
    "\n",
    "# it is bad algotitm to work on \n",
    "# becasie no match of spam =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use another algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import svc\n",
    "svc_model=svc(gamma=\"auto\")\n",
    "svc_model.fit(x_train,Y_train)\n",
    "prediction=svc_model.predict(x_test)\n",
    "print(metrics.confusion_matrix(y_test,prediction))\n",
    "\n",
    "# output will be\n",
    "1373  75\n",
    "121   103\n",
    "\n",
    "# import the model\n",
    "# create object the model\n",
    "# fit te model\n",
    "# predict the model\n",
    "#check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# till no we extract the feature from length and punctuation\n",
    "# but we should also know how to extract  from text message , which is kind of impoetant one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction  from text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most machine learnoing  does not convert text to nummneric\n",
    "# counter vectorization and Term frequency and inverse documnet frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter vaectorization\n",
    "message=[\"Hey how are you\",\"this is crazy talking to you\"]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer()\n",
    "vect.get_feature_names()\n",
    "# output will be\n",
    "[\"call\",\n",
    "\"dogs\"\n",
    ".\n",
    ".\n",
    "]\n",
    "\n",
    "\n",
    "# Term frequncy tf(t,d)\n",
    "\n",
    "#is the raw  count of  term  in a documnet\n",
    "#number of times that term  t occur in document d\n",
    "\n",
    "TF-IDF = term frequncy *(1/document frequncy)\n",
    "TF-IDF = term frequncy * inverse documnet freq\n",
    "\n",
    "Fortunately scikit-learn can calcualte  all these  terms  for us  through the use  of its API\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect=TfidfVectorizer\n",
    "dtm=vect.fit_tranform(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
